{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pulp import *\n",
    "import pulp \n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampos/miniforge3/envs/hawq-test-env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from hawq.utils.quantization_utils.quant_utils import SymmetricQuantFunction, symmetric_linear_quantization_params\n",
    "\n",
    "from models.three_layer import ThreeLayer_BN, ThreeLayerMLP\n",
    "from models.q_three_layer import QThreeLayer_BN\n",
    "\n",
    "from utils.calc_bops import calc_hawq_bops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hawq.utils.export import ExportManager\n",
    "\n",
    "from models.three_layer import ThreeLayerMLP\n",
    "from models.q_three_layer import QThreeLayer\n",
    "from utils.train_utils import config_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ThreeLayerMLP()\n",
    "model.load_state_dict(torch.load(\"/data/jcampos/hawq-jet-tagging/checkpoints/model_best.pth.tar\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [303.83422852 697.54553223 361.45074463  55.21582031]\n",
      "5 [300.52352905 690.93457031 358.4932251   54.71564865]\n",
      "6 [298.87918091 687.21173096 356.55566406  54.51513672]\n",
      "7 [297.98114014 685.22906494 355.17990112  54.42643738]\n",
      "8 [297.51644897 684.15856934 354.45861816  54.38290024]\n"
     ]
    }
   ],
   "source": [
    "# version 4\n",
    "# calculate the L2 norm.\n",
    "# based on HAWQv2 appendix A (symmetric quantization z=0)\n",
    "\n",
    "MIN_BITWIDTH = 4\n",
    "MAX_BITWIDTH = 8\n",
    "LAYERS = [\"dense_1\",\"dense_2\",\"dense_3\",\"dense_4\"]  # for jettagger MLP\n",
    "\n",
    "all_bit_widths = list(range(MIN_BITWIDTH,MAX_BITWIDTH+1))\n",
    "delta_weights = {}  # store the L2 norm. \n",
    "\n",
    "for bit_width in all_bit_widths:\n",
    "    tmp_delta_weights = []\n",
    "    \n",
    "    for idx, layer_name in enumerate(LAYERS):\n",
    "        # get layer to compute quant. error \n",
    "        layer = getattr(model, layer_name)\n",
    "        \n",
    "        # min and max for chosen bitwidth \n",
    "        q_min = -(2**bit_width)\n",
    "        q_max = (2**bit_width)-1\n",
    "\n",
    "        # quantize and dequantize weights\n",
    "        x = torch.clamp(layer.weight, q_min, q_max)\n",
    "        delta = (q_max-q_min)/(q_max-1)\n",
    "        x_integer = torch.round((x-q_min)/delta)\n",
    "        x = x_integer*delta+q_min\n",
    "\n",
    "        # compute the L2 norm. \n",
    "        l2_weight_perturbation = ((x.reshape(1,-1) - layer.weight.reshape(1,-1))**2).sum()\n",
    "        l2_weight_perturbation = l2_weight_perturbation.detach().numpy().item()\n",
    "        \n",
    "        # store resut\n",
    "        tmp_delta_weights.append(l2_weight_perturbation)\n",
    "\n",
    "    delta_weights[bit_width] = np.array(tmp_delta_weights)\n",
    "\n",
    "\n",
    "for bit_width in all_bit_widths:\n",
    "    print(f\"{bit_width} {delta_weights[bit_width]}\")\n",
    "    # print(f\"{bit_width} {delta_weights[bit_width]/np.array([ 1088, 2080, 1056, 165]).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[303.83422852 697.54553223 361.45074463  55.21582031]\n",
      " [300.52352905 690.93457031 358.4932251   54.71564865]\n",
      " [298.87918091 687.21173096 356.55566406  54.51513672]\n",
      " [297.98114014 685.22906494 355.17990112  54.42643738]\n",
      " [297.51644897 684.15856934 354.45861816  54.38290024]]\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array \n",
    "l2_quant_pert = []\n",
    "\n",
    "for bit_width in delta_weights.keys():\n",
    "    l2_quant_pert.append(delta_weights[bit_width])\n",
    "\n",
    "l2_quant_pert = np.array(l2_quant_pert)\n",
    "print(l2_quant_pert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Integer Linear Programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = args\n",
    "args.model_size_limit = 0.5 \n",
    "args.bops_limit = 0.0003 \n",
    "args.latency_limit = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers (for ILP setup)\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "# number of paramers of each layer\n",
    "parameters = np.array([ 1088, 2080, 1056, 165])\n",
    "\n",
    "# Hutchinson_trace means the trace of Hessian for each weight matrix.\n",
    "# These values are already normlized, i.e., Trace / # arameters\n",
    "Hutchinson_trace = np.array([6.451294, 0.8976602, 0.18409997, 0.08547983])  # original (not normalized)\n",
    "\n",
    "# BOPs of each layer\n",
    "bops_2bit = np.array([55296, 47104, 47104, 3520])\n",
    "bops_3bit = np.array([72704, 67584, 32768, 5120])\n",
    "bops_4bit = np.array([90112, 92160, 45056, 7040]) \n",
    "bops_5bit = np.array([107520, 120832, 59392, 9280])\n",
    "bops_6bit = np.array([124928, 153600, 75776, 11840])\n",
    "bops_7bit = np.array([142336, 190464, 94208, 14720])\n",
    "bops_8bit = np.array([159744, 231424, 114688, 17920])\n",
    "bops_9bit = np.array([340992, 215040, 106496, 16640])\n",
    "bops_10bit = np.array([374784, 258048, 128000, 20000])\n",
    "bops_32bit = np.array([1118208, 2240512, 831119232, 174880])\n",
    "\n",
    "# bops = np.array([bops_2bit, bops_3bit, bops_4bit, bops_5bit, bops_6bit, bops_7bit, bops_8bit, bops_9bit, bops_10bit]).reshape(-1) / 1024\n",
    "bops = np.array([bops_4bit, bops_5bit, bops_6bit, bops_7bit, bops_8bit]).reshape(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size \n",
    "model_size_32bit = np.sum(parameters) \n",
    "# model_size_limit = model_size_32bit * args.model_size_limit\n",
    "\n",
    "# bops\n",
    "# BOPS_LIMIT = np.sum(bops_32bit) * args.bops_limit \n",
    "BOPS_LIMIT = int(350e3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - ILP BOPs Size Constrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the problem \n",
    "number_variables = Hutchinson_trace.shape[0]*len(l2_quant_pert) # NUM_LAYERS * BIT_WIDTH_OPTIONS\n",
    "\n",
    "# first get the variables\n",
    "variable = {}\n",
    "for i in range(number_variables):\n",
    "    variable[f\"x{i}\"] = LpVariable(f\"x{i}\", 0, 1, cat=LpInteger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = LpProblem(\"Model_Size\", LpMinimize)\n",
    "\n",
    "# add objective function, minimize model size \n",
    "# prob += sum([variable[f\"x{i}\"] * parameters[i%4] for i in range(num_variable) ]) <= model_size_limit \n",
    "\n",
    "# add objective function, minimize bops \n",
    "prob += sum([variable[f\"x{i}\"] * bops[i] for i in range(number_variables) ]) <= BOPS_LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each layer has BIT_WIDTH_OPTIONS variables (5 in this case), each variable can either be 0 or 1 \n",
    "# an extra constraint is needed to chose one bitwidth per layer\n",
    "prob += sum([variable[f\"x{i}\"] for i in list(range(0, number_variables, NUM_LAYERS))]) == 1  \n",
    "prob += sum([variable[f\"x{i}\"] for i in list(range(1, number_variables, NUM_LAYERS))]) == 1\n",
    "prob += sum([variable[f\"x{i}\"] for i in list(range(2, number_variables, NUM_LAYERS))]) == 1\n",
    "prob += sum([variable[f\"x{i}\"] for i in list(range(3, number_variables, NUM_LAYERS))]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += sum( [ variable[f\"x{i}\"] * l2_quant_pert.reshape(-1)[i] * Hutchinson_trace[i%4] for i in range(number_variables) ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLPSOL--GLPK LP/MIP Solver 5.0\n",
      "Parameter(s) specified in the command line:\n",
      " --cpxlp /tmp/444453f0fe4442c6bc4a1e3f7940e85c-pulp.lp -o /tmp/444453f0fe4442c6bc4a1e3f7940e85c-pulp.sol\n",
      " --tmlim 10000 --simplex\n",
      "Reading problem data from '/tmp/444453f0fe4442c6bc4a1e3f7940e85c-pulp.lp'...\n",
      "5 rows, 20 columns, 40 non-zeros\n",
      "20 integer variables, all of which are binary\n",
      "39 lines were read\n",
      "GLPK Integer Optimizer 5.0\n",
      "5 rows, 20 columns, 40 non-zeros\n",
      "20 integer variables, all of which are binary\n",
      "Preprocessing...\n",
      "5 rows, 20 columns, 40 non-zeros\n",
      "20 integer variables, all of which are binary\n",
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  2.314e+05  ratio =  2.314e+05\n",
      "GM: min|aij| =  7.612e-01  max|aij| =  1.314e+00  ratio =  1.726e+00\n",
      "EQ: min|aij| =  5.805e-01  max|aij| =  1.000e+00  ratio =  1.723e+00\n",
      "2N: min|aij| =  3.438e-01  max|aij| =  1.000e+00  ratio =  2.909e+00\n",
      "Constructing initial basis...\n",
      "Size of triangular part is 5\n",
      "Solving LP relaxation...\n",
      "GLPK Simplex Optimizer 5.0\n",
      "5 rows, 20 columns, 40 non-zeros\n",
      "*     0: obj =   2.615715763e+03 inf =   0.000e+00 (10)\n",
      "*     5: obj =   2.609086261e+03 inf =   0.000e+00 (0)\n",
      "OPTIMAL LP SOLUTION FOUND\n",
      "Integer optimization begins...\n",
      "Long-step dual simplex will be used\n",
      "+     5: mip =     not found yet >=              -inf        (1; 0)\n",
      "Solution found by heuristic: 2610.30897766\n",
      "Solution found by heuristic: 2610.26622307\n",
      "+     9: mip =   2.610266223e+03 >=     tree is empty   0.0% (0; 3)\n",
      "INTEGER OPTIMAL SOLUTION FOUND\n",
      "Time used:   0.0 secs\n",
      "Memory used: 0.1 Mb (60062 bytes)\n",
      "Writing MIP solution to '/tmp/444453f0fe4442c6bc4a1e3f7940e85c-pulp.sol'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Optimal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solve the problem\n",
    "status = prob.solve(GLPK_CMD(msg=1, options=[\"--tmlim\", \"10000\",\"--simplex\"]))\n",
    "# get the result\n",
    "LpStatus[status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape ILP result \n",
    "result = []\n",
    "for i in range(number_variables):\n",
    "    result.append(value(variable[f\"x{i}\"]))\n",
    "\n",
    "result\n",
    "\n",
    "result = np.array(result).reshape(len(l2_quant_pert),-1)\n",
    "bitwidth_idxs = np.argmax(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bit Width\tDense1\t\tDense2\t\tDense3\t\tDense4\n",
      " 4              0               0               0               0\n",
      " 5              0               1               1               1\n",
      " 6              0               0               0               0\n",
      " 7              0               0               0               0\n",
      " 8              1               0               0               0\n",
      "Total BOPs: 349248\n",
      "BOPs limit: 350000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bit Width\\tDense1\\t\\tDense2\\t\\tDense3\\t\\tDense4\")\n",
    "for idx in range(len(l2_quant_pert)):\n",
    "    print(f\"{all_bit_widths[idx]:2} \\\n",
    "          {result[idx][0]:4d} \\\n",
    "            {result[idx][1]:3d} \\\n",
    "            {result[idx][2]:3d} \\\n",
    "            {result[idx][3]:3d}\")\n",
    "\n",
    "# get total bops\n",
    "total_bops = 0\n",
    "for idx in range(NUM_LAYERS):\n",
    "    bops_index = bitwidth_idxs[idx]*NUM_LAYERS + idx\n",
    "    total_bops += bops[bops_index]\n",
    "\n",
    "print(f\"Total BOPs: {total_bops}\")\n",
    "print(f\"BOPs limit: {BOPS_LIMIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f1d7c570f47b2d86ad1fd9b4f66fba2fcbddd70cd883a3ef3509722d52581a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
